version initial content : 

network initialization : orthogonal weight (gain = default) 
				    bias (gain = 0) 

optimizers : Adam actor(lr = 3e-4, betas=(0.9,0.999))
		  critic(lr = 1e-2, betas=(0.9,0.999))

normalization : GAE 

atvantage function : GAE

initialization action std = 0.5

actor network: layer1 (9,256), tanh
	       layer2 (256,128), tanh
	       layer3 (128,64), tanh
	       layer4 (64,1)
critic network: layer1 (1,256), relu
	        layer2 (256,128), relu
	        layer3 (128,64), relu
	        layer4 (64,64), relu
	        layer4 (64,1)

reward function: 
	if abs(yt - self.target) <= self.epsilon : 
            rew = self.constant
        else :
            rew = -(abs(yt - self.target))
        self.rew_array.append(rew)

env reset: action from 0 
	
		
hyperparameters = {
				'timesteps_per_batch': 8000, 
				'max_timesteps_per_episode': 200, 
				'gamma': 0.99, 
				'n_updates_per_iteration': 50,
				'lr': 3e-4, 
				'clip': 0.25,
				'render': True,
				'env_target':1800,
				'env_reward_epsilon':50,
				'env_reward_constant':500,
				'gae_lmbda':0.9,
			  }